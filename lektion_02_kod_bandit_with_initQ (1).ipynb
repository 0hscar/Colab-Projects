{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added Q (i.e initial Q)\n",
    "\n",
    "class Bandit(object):\n",
    "    def __init__(self, numArms, trueRewards, epsilon, mode, Q): #<----\n",
    "        self.Q = [Q for i in range(numArms)]\n",
    "        self.N = [0 for i in range(numArms)]\n",
    "        self.numArms = numArms\n",
    "        self.epsilon = epsilon\n",
    "        self.trueRewards = trueRewards\n",
    "        self.lastAction = None\n",
    "        self.mode = mode  #<----\n",
    "        \n",
    "    def pull(self):\n",
    "        rand = np.random.random()                            # uniform distributed number\n",
    "        if rand <= self.epsilon: \n",
    "            whichArm = np.random.choice(self.numArms)        # compare a random number with epsilon , if smaller we take random action \n",
    "        elif rand > self.epsilon:\n",
    "            a = np.array([approx for approx in self.Q])      # otherwise we take the argmax of the estimates of the wewards\n",
    "            whichArm = np.random.choice(np.where(a == a.max())[0]) # we create a list with the max values, if we get more than one then we choose randomly\n",
    "        \n",
    "        self.lastAction = whichArm                                 # we keep the value of the arm \n",
    "        # self.trueRewards = [reward + 0.1*np.random.randn() for reward in self.trueRewards]  # change in the distribution\n",
    "        \n",
    "        return np.random.randn() + self.trueRewards[whichArm]      # we return a normal distribution reward centered in the true reward for that action\n",
    "\n",
    "    \n",
    "    def updateMean(self,sample):\n",
    "        whichArm = self.lastAction\n",
    "        self.N[whichArm] += 1  \n",
    "    \n",
    "        if self.mode == 'sample-average':\n",
    "            self.Q[whichArm] = self.Q[whichArm] + 1.0/self.N[whichArm] * (sample - self.Q[whichArm])\n",
    "        elif self.mode == 'constant':\n",
    "            self.Q[whichArm] = self.Q[whichArm] + 0.1 * (sample - self.Q[whichArm])  # 0.1 is alpha\n",
    "            \n",
    "            \n",
    "def simulate(numArms, epsilon, numPulls, mode, Q):\n",
    "    rewardHistory = np.zeros(numPulls)\n",
    "    for j in range(2000):\n",
    "        if j % 200 == 0:\n",
    "            print(j)\n",
    "        rewards = [np.random.randn() for _ in range(numActions)]\n",
    "        bandit = Bandit(numArms, rewards, epsilon, mode, Q)\n",
    "        for i in range(numPulls):\n",
    "            reward = bandit.pull()\n",
    "            bandit.updateMean(reward)\n",
    "            rewardHistory[i] += reward\n",
    "    average = rewardHistory / 2000\n",
    "    return average\n",
    "\n",
    "numActions = 10\n",
    "run1 = simulate(numActions, epsilon=0.1, numPulls=1000, mode='sample-average', Q = 0)\n",
    "run2 = simulate(numActions, epsilon=0.0, numPulls=1000, mode='constant', Q = 10)\n",
    "plt.plot(run1, 'b--', run2, 'r--')\n",
    "plt.legend(['realistic epsilon greedy', 'optimistic pure greedy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d6dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
