{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOza1FMXLAw5vKR/4s2ZMXR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Hemläxa 03\n"],"metadata":{"id":"mpyUNmqPJ9qU"}},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWcKHUHlhhkC","executionInfo":{"status":"ok","timestamp":1700480112375,"user_tz":-120,"elapsed":7067,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"e14fdacb-4f71-435d-a12f-06b87c0b8f25"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}]},{"cell_type":"code","source":["# frozen-lake-ex3.py\n","import gymnasium as gym\n","from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n","\n","# Actions in human language\n","actions = {\n","    'Left': 0,\n","    'Down': 1,\n","    'Right': 2,\n","    'Up': 3\n","}\n"],"metadata":{"id":"vrSSBRUQf-mU","executionInfo":{"status":"ok","timestamp":1700480557420,"user_tz":-120,"elapsed":232,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["slippery=False\n","my_policy = ['Right'] + ['Right'] + ['Down'] + ['Down'] + ['Down'] + ['Right'] +['Down'] + ['Right']\n","my_policy_modified = (4*['Right']) + (5*['Down']) + ['Right'] + (2*['Down']) + (2*['Right'])"],"metadata":{"id":"GYr4Q28cgJLP","executionInfo":{"status":"ok","timestamp":1700481432043,"user_tz":-120,"elapsed":2,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def my_frozenlake(slippery, my_policy):\n","\n","    print(\"--- Super amazing policy that will win! ---\")\n","    print(my_policy)\n","    print(\"Maze is slippery: \", slippery)\n","\n","    env = gym.make(\"FrozenLake8x8-v1\", is_slippery=slippery, render_mode=\"human\")\n","    env.reset()\n","    env.render()\n","\n","    for a in my_policy:\n","        new_state, reward, done, info = env.step(actions[a])\n","        print()\n","        env.render()\n","        print(\"Reward: {:.2f}\".format(reward))\n","        print(\"Action:\",a)\n","        print(info)\n","        print(\"Position:\",new_state)\n","        if done:\n","            if reward == 1:\n","                print(\"****You reached the goal!****\")\n","            else:\n","                print(\"****You fell through a hole!****\")\n","            break\n","    env.close()"],"metadata":{"id":"VfIWvCCEhbXy","executionInfo":{"status":"ok","timestamp":1700480923481,"user_tz":-120,"elapsed":1,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Modifierade policy för 8x8\n","my_frozenlake(slippery, my_policy_modified)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dugk5ePhd7V","executionInfo":{"status":"ok","timestamp":1700481454920,"user_tz":-120,"elapsed":4161,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"33e1aa2e-8e18-4e63-d840-192e201c5404"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Super amazing policy that will win! ---\n","['Right', 'Right', 'Right', 'Right', 'Down', 'Down', 'Down', 'Down', 'Down', 'Right', 'Down', 'Down', 'Right', 'Right']\n","Maze is slippery:  False\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 1.0}\n","Position: 1\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 1.0}\n","Position: 2\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 1.0}\n","Position: 3\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 1.0}\n","Position: 4\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 12\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 20\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 28\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 36\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 44\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 1.0}\n","Position: 45\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 53\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 1.0}\n","Position: 61\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 1.0}\n","Position: 62\n","\n","Reward: 1.00\n","Action: Right\n","{'prob': 1.0, 'TimeLimit.truncated': False}\n","Position: 63\n","****You reached the goal!****\n"]}]},{"cell_type":"code","source":["my_frozenlake(True, my_policy_modified)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxzE3XS2heej","executionInfo":{"status":"ok","timestamp":1700481487152,"user_tz":-120,"elapsed":3766,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"e7dd64a6-878f-4432-bb87-edbb05e14f0a"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Super amazing policy that will win! ---\n","['Right', 'Right', 'Right', 'Right', 'Down', 'Down', 'Down', 'Down', 'Down', 'Right', 'Down', 'Down', 'Right', 'Right']\n","Maze is slippery:  True\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 0.3333333333333333}\n","Position: 8\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 0.3333333333333333}\n","Position: 9\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 0.3333333333333333}\n","Position: 10\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 0.3333333333333333}\n","Position: 2\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 1\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 9\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 17\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 18\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 17\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 0.3333333333333333}\n","Position: 25\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 26\n","\n","Reward: 0.00\n","Action: Down\n","{'prob': 0.3333333333333333}\n","Position: 27\n","\n","Reward: 0.00\n","Action: Right\n","{'prob': 0.3333333333333333, 'TimeLimit.truncated': False}\n","Position: 35\n","****You fell through a hole!****\n"]}]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","class QLearningAgent:\n","    def __init__(self, env, alpha=0.5, gamma=0.99, epsilon=10):\n","        self.env = env\n","        self.alpha = alpha  # Learning rate\n","        self.gamma = gamma  # Discount factor\n","        self.epsilon = epsilon  # Exploration rate\n","        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","\n","    def choose_action(self, state):\n","        if np.random.uniform(0, 1) < self.epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            return np.argmax(self.q_table[state])\n","\n","    def learn(self, state, action, reward, next_state):\n","        old_value = self.q_table[state, action]\n","        next_max = np.max(self.q_table[next_state])\n","\n","        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n","        self.q_table[state, action] = new_value\n","\n","    def train(self, episodes=1000):\n","        for episode in range(episodes):\n","            state = self.env.reset()\n","            done = False\n","\n","            while not done:\n","                action = self.choose_action(state)\n","                next_state, reward, done, _ = self.env.step(action)\n","                self.learn(state, action, reward, next_state)\n","                state = next_state\n","                # print(state)\n","\n","    def test(self, episodes=10):\n","        for episode in range(episodes):\n","            state = self.env.reset()\n","            total_reward = 0\n","            done = False\n","\n","            while not done:\n","                action = np.argmax(self.q_table[state])\n","                next_state, reward, done, _ = self.env.step(action)\n","                total_reward += reward\n","                state = next_state\n","\n","            print(f\"Episode {episode + 1} complete. Total reward: {total_reward}\")\n","\n","env = gym.make('FrozenLake8x8-v1', is_slippery=True)\n","\n","agent = QLearningAgent(env)\n","agent.train(episodes=1000)\n","\n","agent.test(episodes=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaDih5dKgJ-l","executionInfo":{"status":"ok","timestamp":1700480590126,"user_tz":-120,"elapsed":2098,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"1a72f89e-bd3a-4d0e-ec69-5b78953a25ec"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1 complete. Total reward: 0.0\n","Episode 2 complete. Total reward: 0.0\n","Episode 3 complete. Total reward: 0.0\n","Episode 4 complete. Total reward: 0.0\n","Episode 5 complete. Total reward: 1.0\n","Episode 6 complete. Total reward: 0.0\n","Episode 7 complete. Total reward: 0.0\n","Episode 8 complete. Total reward: 1.0\n","Episode 9 complete. Total reward: 1.0\n","Episode 10 complete. Total reward: 0.0\n"]}]},{"cell_type":"markdown","source":["Började testa med epsilon 0.1, blev inte till nånting så tog och testade med 10 istället så den får ha lite mera \"fri hand\". Böt även alpha till 0.5 istället för 0.1 jag började med."],"metadata":{"id":"jirnSSMEkoLf"}}]}