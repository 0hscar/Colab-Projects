{"cells":[{"cell_type":"markdown","id":"f2851194","metadata":{"id":"f2851194"},"source":["## GridWorld\n","Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n","\n","### Bibliography:\n","\n","* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n","http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"]},{"cell_type":"code","execution_count":1,"id":"8f1f756d","metadata":{"id":"8f1f756d","executionInfo":{"status":"ok","timestamp":1701290824456,"user_tz":-120,"elapsed":3,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["# imports\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"id":"d07eafa2","metadata":{"id":"d07eafa2","executionInfo":{"status":"ok","timestamp":1701290827842,"user_tz":-120,"elapsed":391,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["# Utilits\n","def printV(V, grid):\n","    for idx, row in enumerate(grid.grid):\n","        for idy, _ in enumerate(row):\n","            state = grid.m * idx + idy\n","            print('%.2f' % V[state], end='\\t')\n","        print('\\n')\n","    print('--------------------')\n","\n","def printPolicy(policy, grid):\n","    for idx, row in enumerate(grid.grid):\n","        for idy, _ in enumerate(row):\n","            state = grid.m * idx + idy\n","            if state in grid.stateSpace:\n","                string = ''.join(policy[state])\n","                print(string, end='\\t')\n","            else:\n","                print('', end='\\t')\n","        print('\\n')\n","    print('--------------------')\n","\n","def printQ(Q, grid):\n","    for idx, row in enumerate(grid.grid):\n","        for idy, _ in enumerate(row):\n","            state = grid.m * idx + idy\n","            if state != grid.m * grid.n - 1:\n","                vals = [np.round(Q[state,action], 5) for action in grid.possibleActions]\n","                print(vals, end='\\t')\n","        print('\\n')\n","    print('--------------------')\n","\n","def sampleReducedActionSpace(grid, action):\n","    actions = grid.possibleActions[:]\n","    actions.remove(action)\n","    sample = np.random.choice(actions)\n","    return sample"]},{"cell_type":"code","execution_count":3,"id":"0ffbb837","metadata":{"id":"0ffbb837","executionInfo":{"status":"ok","timestamp":1701290830173,"user_tz":-120,"elapsed":2,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["class WindyGrid(object):\n","    def __init__(self, m, n, wind):\n","        self.grid = np.zeros((m,n))                            # representation of the grid\n","        self.m = m\n","        self.n = n\n","        self.stateSpace = [i for i in range(self.m*self.n)]\n","        self.stateSpace.remove(28)                              # Terminal state\n","        self.stateSpacePlus = [i for i in range(self.m*self.n)] # State space + terminal state\n","        self.actionSpace = {'U': -self.m, 'D': self.m,\n","                            'L': -1, 'R': 1}\n","        self.possibleActions = ['U', 'D', 'L', 'R']\n","        self.agentPosition = 0\n","        self.wind = wind\n","\n","    def isTerminalState(self, state):\n","        return state in self.stateSpacePlus and state not in self.stateSpace\n","\n","    def getAgentRowAndColumn(self):                               # position of agent\n","        x = self.agentPosition // self.m\n","        y = self.agentPosition % self.n\n","        return x, y\n","\n","    def setState(self, state):\n","        x, y = self.getAgentRowAndColumn()\n","        self.grid[x][y] = 0\n","        self.agentPosition = state\n","        x, y = self.getAgentRowAndColumn()\n","        self.grid[x][y] = 1\n","\n","    def offGridMove(self, newState, oldState):\n","        # if we move into a row not in the grid\n","        if newState not in self.stateSpacePlus:\n","            return True\n","        # if we're trying to wrap around to next row\n","        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n","            return True\n","        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n","            return True\n","        else:\n","            return False\n","\n","    # Include wind stenght.\n","    def step(self, action):\n","        agentX, agentY = self.getAgentRowAndColumn()\n","        if agentX > 0:\n","            resultingState = self.agentPosition + self.actionSpace[action] + \\\n","                            self.wind[agentY] * self.actionSpace['U']\n","            if resultingState < 0: #if the wind is trying to push agent off grid\n","                resultingState += self.m\n","        else:\n","            if action == 'L' or action == 'R':\n","                resultingState = self.agentPosition + self.actionSpace[action]\n","            else:\n","                resultingState = self.agentPosition + self.actionSpace[action] + \\\n","                            self.wind[agentY] * self.actionSpace['U']\n","        #reward = -1 if not self.isTerminalState(resultingState) else 0\n","        reward = -1\n","        if not self.offGridMove(resultingState, self.agentPosition):\n","            self.setState(resultingState)\n","            return resultingState, reward, self.isTerminalState(resultingState), None\n","        else:\n","            return self.agentPosition, reward, self.isTerminalState(self.agentPosition), None\n","\n","    def reset(self):\n","        self.agentPosition = 0\n","        self.grid = np.zeros((self.m,self.n))\n","        return self.agentPosition, False\n","\n","\n","    def render(self):\n","        print('------------------------------------------')\n","        for row in self.grid:\n","            for col in row:\n","                if col == 0:\n","                    print('-', end='\\t')\n","                elif col == 1:\n","                    print('X', end='\\t')\n","            print('\\n')\n","        print('------------------------------------------')\n",""]},{"cell_type":"markdown","id":"4216e193","metadata":{"id":"4216e193"},"source":["## First visit Monte Carlo Prediction"]},{"cell_type":"code","execution_count":15,"id":"2387e21b","metadata":{"id":"2387e21b","executionInfo":{"status":"ok","timestamp":1701292317637,"user_tz":-120,"elapsed":236,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["def MC_first_visit(loop=500, size=6, GAMMA=1.0, wind=[0, 0, 1, 2, 1, 0], part=0):\n","\n","\n","    grid = WindyGrid(size,size, wind)\n","    # GAMMA = 1.0\n","\n","    policy = {}                              #  a dictionary that maps each\n","    for state in grid.stateSpace:            #  state to the list of possible actions\n","        policy[state] = grid.possibleActions\n","\n","    V = {}                                   # Initialize our initial estimate of the value\n","    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n","        V[state] = 0\n","\n","    returns = {}                             #Initialize a dictionary that keeps a list\n","    for state in grid.stateSpace:            #of the returns for each state.\n","        returns[state] = []\n","\n","    for i in range(loop):                     # Loop over 500 games,\n","        observation, done = grid.reset()     # resetting the grid and memory with each game.\n","        memory = []                          # empty list to keep track of the states visited\n","        statesReturns = []                   # and returns at each time step\n","        if i % 100 == 0:                     # Just to know if the game is running.\n","            print('starting episode', i)\n","        while not done:                      # While the game isn't done\n","            # attempt to follow the policy. In this case choose an action\n","            # according to the random equiprobable strategy.\n","            action = np.random.choice(policy[observation])\n","            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n","            memory.append((observation, action, reward))\n","            observation = observation_\n","\n","        # append terminal state\n","        memory.append((observation, action, reward))\n","\n","        G = 0                                  # set G=0\n","        last = True                            # initialize a Boolean to keep track of the visit to the last state\n","        for state, action, reward in reversed(memory):\n","            if last:\n","                last = False\n","            else:                                    # Skip the terminal state and append the set of states\n","                statesReturns.append((state,G))      #  and returns to the statesReturns list.\n","            G = GAMMA*G + reward\n","\n","        statesReturns.reverse()                  # to ge it in chronological order\n","        statesVisited = []                       # keep track of the visited states during the episode.\n","        for state, G in statesReturns:\n","            if state not in statesVisited:       # Iterate over the episode and see\n","                returns[state].append(G)         # if each state has been visited before.\n","                V[state] = np.mean(returns[state])\n","                statesVisited.append(state)\n","\n","                #If it hasn't, meaning this is the agent's first visit, go ahead and append\n","                #the returns to the returns dictionary for that state.\n","                #Calculate the value function by taking the mean of the returns for that state, and finally,\n","                #append that state to the list of statesVisited.\n","    print(\"\\n\")\n","    print(part)\n","    printV(V, grid)"]},{"cell_type":"markdown","id":"42c0c1eb","metadata":{"id":"42c0c1eb"},"source":["## Del 1:\n","\n","- Anv칛nd *first visit* Monte Carlo Metoden\n","\n","1. 칐ka vindstyrkan med en enhet.\n","    - Hur 칛ndras slutv칛rdesfunktionen?\n","\n","\n","2. Hur 칛ndras v칛rdefunktion om man 칛ndra gamma till:\n","    - 洧=0.5\n","    - 洧=0,9\n","    - 洧=0,95\n","\n","\n","3. Testa rutn칛tsv칛rlden i storlekarna:\n","    - 8x8\n","        - 츿ndra p친 vinden, vad h칛nder med v칛rdefunktion?\n","        - Prova med 洧=0,9, vad h칛nder med v칛rdefunktion?\n","    - 10x10\n","        - 츿ndra p친 vinden, vad h칛nder med v칛rdefunktion?\n","        - Prova med 洧=0,9, vad h칛nder med v칛rdefunktion?"]},{"cell_type":"code","execution_count":16,"id":"2885845a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2885845a","executionInfo":{"status":"error","timestamp":1701292522740,"user_tz":-120,"elapsed":89346,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"2da69299-707d-4007-db66-b41d78508082"},"outputs":[{"output_type":"stream","name":"stdout","text":["starting episode 0\n","starting episode 100\n","starting episode 200\n","starting episode 300\n","starting episode 400\n","\n","\n","0\n","-1523.21\t-1522.51\t-1509.65\t-1496.10\t-1481.04\t-1460.69\t\n","\n","-1531.58\t-1525.28\t-1504.53\t-1468.74\t-1483.62\t-1438.26\t\n","\n","-1526.62\t-1528.63\t-1494.21\t-1451.67\t-1469.20\t-1362.64\t\n","\n","-1510.49\t-1499.59\t-1486.08\t-1462.22\t-1375.64\t-1177.50\t\n","\n","-1478.52\t-1464.60\t-1451.62\t-1474.60\t0.00\t-748.42\t\n","\n","-1463.66\t-1457.46\t-1433.27\t0.00\t-1218.66\t-959.54\t\n","\n","--------------------\n","starting episode 0\n","starting episode 100\n","starting episode 200\n","starting episode 300\n","starting episode 400\n","\n","\n","2\n","-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n","\n","-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n","\n","-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.99\t\n","\n","-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.95\t\n","\n","-2.00\t-2.00\t-2.00\t-2.00\t0.00\t-1.68\t\n","\n","-2.00\t-2.00\t-2.00\t0.00\t-1.95\t-1.93\t\n","\n","--------------------\n","starting episode 0\n","starting episode 100\n","starting episode 200\n","starting episode 300\n","starting episode 400\n","\n","\n","3\n","-10.00\t-10.00\t-9.99\t-9.98\t-9.98\t-9.94\t\n","\n","-10.00\t-10.00\t-9.99\t-9.98\t-9.96\t-9.86\t\n","\n","-10.00\t-10.00\t-9.98\t-9.99\t-9.90\t-9.61\t\n","\n","-10.00\t-10.00\t-9.99\t-9.99\t-9.90\t-8.76\t\n","\n","-10.00\t-10.00\t-9.99\t-9.99\t0.00\t-6.15\t\n","\n","-10.00\t-10.00\t-9.99\t0.00\t-8.90\t-8.11\t\n","\n","--------------------\n","starting episode 0\n","starting episode 100\n","starting episode 200\n","starting episode 300\n","starting episode 400\n","\n","\n","4\n","-19.95\t-19.95\t-19.92\t-19.88\t-19.78\t-19.68\t\n","\n","-19.95\t-19.94\t-19.90\t-19.84\t-19.76\t-19.52\t\n","\n","-19.91\t-19.93\t-19.89\t-19.83\t-19.69\t-18.59\t\n","\n","-19.93\t-19.92\t-19.91\t-19.89\t-19.33\t-16.51\t\n","\n","-19.95\t-19.94\t-19.86\t-19.76\t0.00\t-11.27\t\n","\n","-19.93\t-19.94\t-19.89\t0.00\t-17.55\t-14.22\t\n","\n","--------------------\n","starting episode 0\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-ba4b3e82b470>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 3 a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mMC_first_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Default 8x8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# MC_first_visit(500, 8, 1.0, [1, 1, 2, 3, 2, 1], 6) # Wind increase 8x8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mMC_first_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Gamma increase 8x8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-99bb7e0916ae>\u001b[0m in \u001b[0;36mMC_first_visit\u001b[0;34m(loop, size, GAMMA, wind, part)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# according to the random equiprobable strategy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Take that action, get new state, reward and done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-8012e5d9f913>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mresultingState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magentPosition\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionSpace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magentY\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionSpace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m#reward = -1 if not self.isTerminalState(resultingState) else 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["MC_first_visit() # Default\n","\n","# 1\n","# MC_first_visit(500, 6, 1.0, [1,1,2,3,2,1], 1) # Wind increase. It took forever to progress with wind changed, do not know why. It's the same for them all\n","\n","# 2\n","MC_first_visit(500, 6, 0.5, [0, 0, 1, 2, 1, 0], 2) # Gamma 0.5\n","MC_first_visit(500, 6, 0.9, [0, 0, 1, 2, 1, 0], 3) # Gamma 0.9\n","MC_first_visit(500, 6, 0.95, [0, 0, 1, 2, 1, 0], 4) # Gamma 0.95\n","\n","# 3 a\n","MC_first_visit(500, 8, 1.0, [0, 0, 1, 2, 1, 0], 5) # Default 8x8\n","# MC_first_visit(500, 8, 1.0, [1, 1, 2, 3, 2, 1], 6) # Wind increase 8x8\n","MC_first_visit(500, 8, 0.9, [0, 0, 1, 2, 1, 0], 7) # Gamma increase 8x8\n","\n","# 3 b\n","MC_first_visit(500, 10, 1.0, [0, 0, 1, 2, 1, 0], 8) # Default 10x10\n","# MC_first_visit(500, 10, 1.0, [1, 1, 2, 3, 2, 1], 9) # Wind increase 10x10\n","# MC_first_visit(500, 10, 0.9, [1, 1, 2, 3, 2, 1], 10) # Wind increase 10x10\n","\n"]},{"cell_type":"code","execution_count":null,"id":"bb6fc670","metadata":{"id":"bb6fc670"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"efb5c2d2","metadata":{"id":"efb5c2d2"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2be5ffb9","metadata":{"id":"2be5ffb9"},"source":["## Exploring Start Monte Carlo"]},{"cell_type":"code","execution_count":null,"id":"428bf8de","metadata":{"id":"428bf8de"},"outputs":[],"source":["def MC_exploring_starts(size=6, GAMMA=0.9, wind=[0, 0, 1, 2, 1, 0], part=0):\n","\n","    grid = WindyGrid(size,size, wind)\n","    # GAMMA = 0.9\n","\n","    # Initialize Q, returns, and pairs visited\n","    Q = {}\n","    returns = {}\n","    pairsVisited = {}\n","    for state in grid.stateSpacePlus:\n","        for action in grid.possibleActions:\n","            Q[(state, action)] = 0\n","            returns[(state,action)] = 0\n","            pairsVisited[(state,action)] = 0\n","\n","    # initialize a random policy\n","    policy = {}\n","    for state in grid.stateSpace:\n","        policy[state] = np.random.choice(grid.possibleActions)\n","\n","    for i in range(1000000):\n","        if i % 50000 == 0:\n","            print('starting episode', i)\n","        statesActionsReturns = []\n","        observation = np.random.choice(grid.stateSpace)\n","        action = np.random.choice(grid.possibleActions)\n","        grid.setState(observation)\n","        observation_, reward, done, info = grid.step(action)\n","        memory = [(observation, action, reward)]\n","        steps = 1\n","        while not done:\n","            action = policy[observation_]\n","            steps += 1\n","            observation, reward, done, info = grid.step(action)\n","            if steps > 15 and not done:\n","                done = True\n","                reward = -steps\n","            memory.append((observation_, action, reward))\n","            observation_ = observation\n","\n","        # append the terminal state\n","        memory.append((observation_, action, reward))\n","\n","        G = 0\n","        last = True # start at t = T - 1\n","        for state, action, reward in reversed(memory):\n","            if last:\n","                last = False\n","            else:\n","                statesActionsReturns.append((state,action, G))\n","            G = GAMMA*G + reward\n","\n","        statesActionsReturns.reverse()\n","        statesAndActions = []\n","        for state, action, G in statesActionsReturns:\n","            if (state, action) not in statesAndActions:\n","                pairsVisited[(state,action)] += 1\n","                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n","                Q[(state,action)] = returns[(state,action)]\n","                statesAndActions.append((state,action))\n","                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n","                best = np.argmax(values)\n","                policy[state] = grid.possibleActions[best]\n","\n","    printQ(Q, grid)\n","    printPolicy(policy,grid)"]},{"cell_type":"markdown","id":"9298a493","metadata":{"id":"9298a493"},"source":["## Del 2\n","\n","\n","- Anv칛nd  *exploring starts* Monte Carlo Metoden\n","\n","1. 칐ka vindstyrkan med en enhet.\n","    - Hur 칛ndras slutv칛rdesfunktionen?\n","\n","\n","2. Hur 칛ndras policyn om man 칛ndra gamma till:\n","    - 洧=0.5\n","    - 洧=0,9\n","    - 洧=0,95\n","\n","\n","3. Testa rutn칛tsv칛rlden i storlekarna:\n","    - 8x8\n","        - 츿ndra p친 vinden, vad h칛nder med policyn?\n","        - Prova med 洧=0,9, vad h칛nder med policyn?\n","    - 10x10\n","        - 츿ndra p친 vinden, vad h칛nder med policyn?"]},{"cell_type":"code","execution_count":null,"id":"42bdbc66","metadata":{"id":"42bdbc66"},"outputs":[],"source":["MC_exploring_starts(6, 0.9, [0, 0, 1, 2, 1, 0]) #  Default\n","MC_exploring_starts(6, 0.9, [0, 0, 1, 2, 1, 0], 1) #\n"]},{"cell_type":"code","execution_count":null,"id":"a7f92dc5","metadata":{"id":"a7f92dc5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fcd0efc3","metadata":{"id":"fcd0efc3"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"054b42a6","metadata":{"id":"054b42a6"},"source":["## On-policy first visit Monte Carlo for $\\varepsilon$-soft policies"]},{"cell_type":"code","execution_count":null,"id":"ba2dcfa7","metadata":{"id":"ba2dcfa7"},"outputs":[],"source":["def MC_without_exploring_starts():\n","    grid = WindyGrid(6,6, wind=[0, 0, 1, 2, 1, 0])\n","    GAMMA = 0.9\n","    EPS = 0.4\n","\n","    Q = {}\n","    returns = {}\n","    pairsVisited = {}\n","    for state in grid.stateSpacePlus:\n","        for action in grid.actionSpace.keys():\n","            Q[(state, action)] = 0\n","            returns[(state,action)] = 0\n","            pairsVisited[(state,action)] = 0\n","\n","    policy = {}\n","    for state in grid.stateSpace:\n","        policy[state] = grid.possibleActions\n","\n","    for i in range(1000000):\n","        statesActionsReturns = []\n","        if i % 100000 == 0:\n","            print('starting episode', i)\n","        observation, done = grid.reset()\n","        memory = []\n","        steps = 0\n","        while not done:\n","            if len(policy[observation]) > 1:\n","                action = np.random.choice(policy[observation])\n","            else:\n","                action = policy[observation]\n","            observation_, reward, done, info = grid.step(action)\n","            steps += 1\n","            if steps > 25 and not done:\n","                done = True\n","                reward = -steps\n","            memory.append((observation, action, reward))\n","            observation = observation_\n","\n","        #append the terminal state\n","        memory.append((observation, action, reward))\n","\n","        G = 0\n","        last = True # start at t = T - 1\n","        for state, action, reward in reversed(memory):\n","            if last:\n","                last = False\n","            else:\n","                statesActionsReturns.append((state,action,G))\n","            G = GAMMA*G + reward\n","        statesActionsReturns.reverse()\n","\n","        statesAndActions = []\n","        for state, action, G in statesActionsReturns:\n","            if (state, action) not in statesAndActions:\n","                pairsVisited[(state,action)] += 1\n","                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])\n","                Q[(state,action)] = returns[(state,action)]\n","                statesAndActions.append((state,action))\n","                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n","                best = np.random.choice(np.where(values==values.max())[0])\n","                rand = np.random.random()\n","                if rand < 1 - EPS:\n","                    policy[state] = grid.possibleActions[best]\n","                else:\n","                    policy[state] = np.random.choice(grid.possibleActions)\n","\n","    printQ(Q, grid)\n","    printPolicy(policy,grid)"]},{"cell_type":"markdown","id":"88293b06","metadata":{"id":"88293b06"},"source":["## Del 3\n","- Anv칛nd *without exploring starts* Monte Carlo Metoden\n","\n","1. 칐ka vindstyrkan med en enhet.\n","    - Hur 칛ndras slutv칛rdesfunktionen?\n","\n","\n","2. Hur 칛ndras policyn om man 칛ndra gamma till:\n","    - 洧=0.5\n","    - 洧=0,9\n","    - 洧=0,95\n","\n","\n","3. Testa rutn칛tsv칛rlden i storlekarna:\n","    - 8x8\n","        - 츿ndra p친 vinden, vad h칛nder med policyn?\n","        - Prova med 洧=0,9, vad h칛nder med policyn?\n","    - 10x10\n","        - 츿ndra p친 vinden, vad h칛nder med policyn?"]},{"cell_type":"code","execution_count":null,"id":"d2dc828a","metadata":{"id":"d2dc828a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"a1a3d19a","metadata":{"id":"a1a3d19a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b0509f9a","metadata":{"id":"b0509f9a"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"4d1a2a0d","metadata":{"id":"4d1a2a0d"},"source":["## Off-Policy Monte Carlo prediction"]},{"cell_type":"code","execution_count":null,"id":"dc7f57b8","metadata":{"id":"dc7f57b8"},"outputs":[],"source":["def MC_off_policy_prediction():\n","    grid = WindyGrid(6,6, wind=[0,0,1,2,1,0])\n","    GAMMA = 0.9\n","\n","    Q = {}\n","    C = {}\n","    for state in grid.stateSpacePlus:\n","        for action in grid.possibleActions:\n","            Q[(state,action)] = 0\n","            C[(state,action)] = 0\n","\n","    targetPolicy = {}\n","    for state in grid.stateSpace:\n","        targetPolicy[state] = np.random.choice(grid.possibleActions)\n","\n","    for i in range(1000000):\n","        if i % 100000 == 0:\n","            print(i)\n","        behaviorPolicy = {}\n","        for state in grid.stateSpace:\n","            behaviorPolicy[state] = grid.possibleActions\n","        memory = []\n","        observation, done = grid.reset()\n","        steps = 0\n","        while not done:\n","            action = np.random.choice(behaviorPolicy[observation])\n","            observation_, reward, done, info = grid.step(action)\n","            steps += 1\n","            if steps > 25:\n","                done = True\n","                reward = -steps\n","            memory.append((observation, action, reward))\n","            observation = observation_\n","        memory.append((observation, action, reward))\n","\n","        G = 0\n","        W = 1\n","        last = True\n","        for (state, action, reward) in reversed(memory):\n","            if last:\n","                last = False\n","            else:\n","                C[state,action] += W\n","                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n","                prob = 1 if action in targetPolicy[state] else 0\n","                W *= prob/(1/len(behaviorPolicy[state]))\n","                if W == 0:\n","                    break\n","            G = GAMMA*G + reward\n","    printQ(Q, grid)\n","    printPolicy(targetPolicy,grid)"]},{"cell_type":"markdown","id":"e27b9cc6","metadata":{"id":"e27b9cc6"},"source":["## Del 4\n","- Anv칛nd *off-policy prediction* Monte Carlo Metoden\n","\n","1. 칐ka vindstyrkan med en enhet.\n","    - Hur 칛ndras slutv칛rdesfunktionen?\n","\n","\n","2. Hur 칛ndras policyn om man 칛ndra gamma till:\n","    - 洧=0.5\n","    - 洧=0,9\n","    - 洧=0,95\n","\n","\n","3. Testa rutn칛tsv칛rlden i storlekarna:\n","    - 8x8\n","        - 츿ndra p친 vinden, vad h칛nder med policyn?\n","        - Prova med 洧=0,9, vad h칛nder med policyn?\n","    - 10x10\n","        - 츿ndra p친 vinden, vad h칛nder med policyn?"]},{"cell_type":"code","execution_count":null,"id":"406a1b42","metadata":{"id":"406a1b42"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"560a06c5","metadata":{"id":"560a06c5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"455c795f","metadata":{"id":"455c795f"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}