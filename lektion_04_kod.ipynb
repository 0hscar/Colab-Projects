{"cells":[{"cell_type":"markdown","id":"ccc3471e","metadata":{"id":"ccc3471e"},"source":["## GridWorld\n","Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n","\n","### Bibliography:\n","\n","* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n","http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"]},{"cell_type":"code","execution_count":1,"id":"1d331c1c","metadata":{"id":"1d331c1c","executionInfo":{"status":"ok","timestamp":1700226498539,"user_tz":-120,"elapsed":2,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["# imports\n","import numpy as np\n","from IPython.display import clear_output\n","import time"]},{"cell_type":"code","execution_count":2,"id":"dab2812c","metadata":{"id":"dab2812c","executionInfo":{"status":"ok","timestamp":1700226498539,"user_tz":-120,"elapsed":2,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["class GridWorld(object):\n","    \"\"\" Gridworld defined by m x n matrix with\n","    terminal states at top left corner and bottom right corner.\n","    State transitions are deterministic; attempting to move\n","    off the grid leaves the state unchanged, and rewards are -1 on\n","    each step.\n","\n","    In this implementation we model the environment like a game\n","    where an agent moves around.\n","    \"\"\"\n","    def __init__(self, m, n):\n","        self.m = m\n","        self.n = n\n","        self.grid = np.zeros((m,n))\n","        self.stateSpace = [i+1 for i in range(self.m*self.n-2)]\n","        self.stateSpacePlus = [i for i in range(self.m*self.n)]\n","        self.actionSpace = {'up': -self.m, 'down': self.m,\n","                            'left': -1, 'right': 1}\n","        self.p = self.initP() # probability functions\n","        self.agentPosition = np.random.choice(self.stateSpace)\n","        x, y = self.getAgentRowAndColumn()\n","        self.grid[x][y] = 1\n","\n","    def getAgentRowAndColumn(self):\n","        x = self.agentPosition // self.m\n","        y = self.agentPosition % self.n\n","        return x, y\n","\n","    def setState(self, state):\n","        x, y = self.getAgentRowAndColumn()\n","        self.grid[x][y] = 0\n","        self.agentPosition = state\n","        x, y = self.getAgentRowAndColumn()\n","        self.grid[x][y] = 1\n","\n","    def offGridMove(self, newState, oldState):\n","        # if we move into a row not in the grid\n","        if newState not in self.stateSpacePlus:\n","            return True\n","        # if we're trying to wrap around to next row\n","        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n","            return True\n","        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n","            return True\n","        else:\n","            return False\n","    def step(self, action):\n","        resultingState = self.agentPosition + self.actionSpace[action]\n","        if not self.offGridMove(resultingState, self.agentPosition):\n","            self.setState(resultingState)\n","            return resultingState, -1, self.isTerminalState(resultingState), None\n","        else:\n","            return self.agentPosition, -1, self.isTerminalState(self.agentPosition), None\n","\n","    def isTerminalState(self, state):\n","        return state in self.stateSpacePlus and state not in self.stateSpace\n","\n","    def initP(self):\n","        \"\"\" construct state transition probabilities for\n","        use in value function. P(s', r|s, a) is a dictionary\n","        with keys corresponding to the functional arguments.\n","        values are either 1 or 0.\n","        Translations that take agent off grid leave the state unchanged.\n","        (s', r|s, a)\n","        (1, -1|1, 'up') = 1\n","        (1, -1|2, 'left') = 1\n","        (1, -1|3, 'left') = 0\n","        \"\"\"\n","        P = {}\n","        for state in self.stateSpace:\n","            for action in self.actionSpace:\n","                resultingState = state + self.actionSpace[action]\n","                key = (state, -1, state, action) if self.offGridMove(resultingState, state) \\\n","                                                 else (resultingState, -1, state, action)\n","                P[key] = 1\n","        return P\n","\n","    def render(self):\n","        print('------------------------------------------')\n","        for row in self.grid:\n","            for col in row:\n","                if col == 0:\n","                    print('-', end='\\t')\n","                elif col == 1:\n","                    print('X', end='\\t')\n","            print('\\n')\n","        print('------------------------------------------')"]},{"cell_type":"code","execution_count":3,"id":"5b9a7319","metadata":{"id":"5b9a7319","executionInfo":{"status":"ok","timestamp":1700226518183,"user_tz":-120,"elapsed":251,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"5224d386-f65e-4dc2-a8c0-8d2bb0b5c3ab","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------\n","-\tX\t-\t-\t\n","\n","-\t-\t-\t-\t\n","\n","-\t-\t-\t-\t\n","\n","-\t-\t-\t-\t\n","\n","------------------------------------------\n"]}],"source":["# Skapa ett rutnät\n","grid = GridWorld(4,4)\n","# Se på rutnätet - Agenten är X\n","grid.render()"]},{"cell_type":"markdown","id":"c15bbc5f","metadata":{"id":"c15bbc5f"},"source":["## Del 1 - Utvärdera policyn"]},{"cell_type":"code","execution_count":4,"id":"6ca77fd1","metadata":{"id":"6ca77fd1","executionInfo":{"status":"ok","timestamp":1700226519955,"user_tz":-120,"elapsed":1,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["def evaluatePolicy(grid, V, policy, GAMMA, THETA):\n","    # policy evaluation for the random choice in gridworld\n","    converged = False\n","    iterations = 0 # <---\n","    while not converged:\n","        DELTA = 0\n","        for state in grid.stateSpace:\n","            oldV = V[state]\n","            total = 0\n","            weight = 1 / len(policy[state])\n","            for action in policy[state]:\n","                grid.setState(state)\n","                newState, reward, _, _ = grid.step(action)\n","                key = (newState, reward, state, action)\n","                total += weight*grid.p[key]*(reward+GAMMA*V[newState])\n","            V[state] = total\n","            DELTA = max(DELTA, np.abs(oldV-V[state]))\n","            converged = True if DELTA < THETA else False\n","        iterations = iterations + 1 # <---\n","    print('iterations: ', iterations) # <----\n","    return V"]},{"cell_type":"code","execution_count":5,"id":"8e5f8377","metadata":{"id":"8e5f8377","executionInfo":{"status":"ok","timestamp":1700226522569,"user_tz":-120,"elapsed":216,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["def printV(V, grid):\n","    for idx, row in enumerate(grid.grid):\n","        for idy, _ in enumerate(row):\n","            state = grid.m * idx + idy\n","            print('%.2f' % V[state], end='\\t')\n","        print('\\n')\n","    print('--------------------')"]},{"cell_type":"code","execution_count":6,"id":"6057c53d","metadata":{"id":"6057c53d","executionInfo":{"status":"ok","timestamp":1700226524288,"user_tz":-120,"elapsed":1,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}}},"outputs":[],"source":["# Körning med evaluatePolicy\n","def get_policy(m, n, x, y):\n","    grid = GridWorld(m,n) # Rutnät m x n\n","    THETA = x # Konvergensvillkor\n","    GAMMA = y\n","\n","    # initialize V(s)\n","    V = {}\n","    for state in grid.stateSpacePlus:\n","        V[state] = 0\n","\n","    policy = {}\n","    for state in grid.stateSpace:\n","        policy[state] = [key for key in grid.actionSpace.keys()]\n","\n","\n","    V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n","    printV(V, grid)"]},{"cell_type":"code","execution_count":7,"id":"4088e4ad","metadata":{"id":"4088e4ad","executionInfo":{"status":"ok","timestamp":1700226526522,"user_tz":-120,"elapsed":375,"user":{"displayName":"Oscar Weber","userId":"12113599767368361142"}},"outputId":"d8bac1d5-1c7a-4f60-fc27-a512943e62b3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["iterations:  114\n","0.00\t-14.00\t-20.00\t-22.00\t\n","\n","-14.00\t-18.00\t-20.00\t-20.00\t\n","\n","-20.00\t-20.00\t-18.00\t-14.00\t\n","\n","-22.00\t-20.00\t-14.00\t0.00\t\n","\n","--------------------\n"]}],"source":["# Körning med get_policy\n","get_policy(4, 4, 0.0001, 1.0)"]},{"cell_type":"markdown","id":"00454e56","metadata":{"id":"00454e56"},"source":["## Del 2 - Förbättra policyn"]},{"cell_type":"code","execution_count":null,"id":"63c1378e","metadata":{"id":"63c1378e"},"outputs":[],"source":["def improvePolicy(grid, V, policy, GAMMA):\n","    stable = True\n","    newPolicy = {}\n","    for state in grid.stateSpace:\n","        oldActions = policy[state]\n","        value = []\n","        newAction = []\n","        for action in policy[state]:\n","            grid.setState(state)\n","            weight = 1 / len(policy[state])\n","            newState, reward, _, _ = grid.step(action)\n","            key = (newState, reward, state, action)\n","            value.append(np.round(weight*grid.p[key]*(reward+GAMMA*V[newState]), 2))\n","            newAction.append(action)\n","        value = np.array(value)\n","        best = np.where(value == value.max())[0]\n","        bestActions = [newAction[item] for item in best]\n","        newPolicy[state] = bestActions\n","\n","        if oldActions != bestActions:\n","            stable = False\n","\n","    return stable, newPolicy"]},{"cell_type":"code","execution_count":null,"id":"6adc0a16","metadata":{"id":"6adc0a16"},"outputs":[],"source":["def get_policy_iteration(m, n, x, y):\n","\n","    grid = GridWorld(m,n)\n","    THETA = x\n","    GAMMA = y\n","\n","    # initialize V(s)\n","\n","    V = {}\n","    for state in grid.stateSpacePlus:\n","        V[state] = 0\n","\n","    policy = {}\n","    for state in grid.stateSpace:\n","        policy[state] = [key for key in grid.actionSpace.keys()]\n","\n","    # main loop for policy improvement\n","    stable = False\n","    while not stable:\n","        V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n","        stable, policy = improvePolicy(grid, V, policy, GAMMA)\n","        V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n","\n","        printV(V, grid)\n","        time.sleep(2)\n","        clear_output(wait=True)\n","\n","    printV(V,grid)\n","\n","    for state in policy:\n","        print(state,policy[state])"]},{"cell_type":"code","execution_count":null,"id":"93edec92","metadata":{"id":"93edec92"},"outputs":[],"source":["# Körning med improvePolicy\n","get_policy_iteration(4, 4, 10e-6, 1.0)"]},{"cell_type":"markdown","id":"8d888545","metadata":{"id":"8d888545"},"source":["## Del 3 - Värde iteration"]},{"cell_type":"code","execution_count":null,"id":"dd37880a","metadata":{"id":"dd37880a"},"outputs":[],"source":["def iterateValues(grid, V, policy, GAMMA, THETA):\n","    converged = False\n","    while not converged:\n","        DELTA = 0\n","        for state in grid.stateSpace:\n","            oldV = V[state]\n","            newV = []\n","            for action in grid.actionSpace:\n","                grid.setState(state)\n","                newState, reward, _, _ = grid.step(action)\n","                key = (newState, reward, state, action)\n","                newV.append(grid.p[key]*(reward+GAMMA*V[newState]))\n","            newV = np.array(newV)\n","            bestV = np.where(newV == newV.max())[0]\n","            bestState = np.random.choice(bestV)\n","            V[state] = newV[bestState]\n","            DELTA = max(DELTA, np.abs(oldV-V[state]))\n","            converged = True if DELTA < THETA else False\n","\n","    for state in grid.stateSpace:\n","        newValues = []\n","        actions = []\n","        for action in grid.actionSpace:\n","            grid.setState(state)\n","            newState, reward, _, _ = grid.step(action)\n","            key = (newState, reward, state, action)\n","            newValues.append(grid.p[key]*(reward+GAMMA*V[newState]))\n","            actions.append(action)\n","        newValues = np.array(newValues)\n","        bestActionIDX = np.where(newValues == newValues.max())[0]\n","        #bestActions = actions[np.random.choice(bestActionIDX)]\n","        bestActions = actions[bestActionIDX[0]]\n","        policy[state] = bestActions\n","\n","    return V, policy"]},{"cell_type":"code","execution_count":null,"id":"4cd10bf5","metadata":{"id":"4cd10bf5"},"outputs":[],"source":["def get_value_iteration(m, n, x, y):\n","\n","    grid = GridWorld(m,n)\n","    THETA = x\n","    GAMMA = y\n","\n","    # initialize V(s)\n","    V = {}\n","    for state in grid.stateSpacePlus:\n","        V[state] = 0\n","\n","    policy = {}\n","    for state in grid.stateSpace:\n","        policy[state] = [key for key in grid.actionSpace.keys()]\n","\n","    for i in range(2):\n","        V, policy = iterateValues(grid, V, policy, GAMMA, THETA)\n","        printV(V, grid)\n","        time.sleep(2)\n","        clear_output(wait=True)\n","    # Final\n","\n","    printV(V, grid)\n","    print()\n","    for state in policy:\n","        print(state, policy[state])"]},{"cell_type":"code","execution_count":null,"id":"40316e57","metadata":{"id":"40316e57"},"outputs":[],"source":["# körning med iterateValues\n","get_value_iteration(4, 4, 10e-6, 1.0)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}